#### 6.1.3.4. Label Assignment

这部分就着重探讨一下前述的回归的思路应该怎么样进行标签分配。动态标签分配被OTA带火之后已经算是检测网络的标配，不管用在什么架构上都能涨点，对于我们的场景也同样重要。标签分配通过选出那些优质的预测结果，将GT分配给它们从而实现优者更优。不用担心会有anchor无法接受正样本的监督，神经网络超强的抗噪能力以及丰富的数据增强，加上前层的参数共享都能让各个anchor逐渐学习到最适合自己预测的目标。

>  基于heatmap的方法不进行需要标签分配，因为每个位置输出的就是该点存在特征点的概率。用的最多的方法就是让预测的分布和GT分布尽可能一致，选用一些度量分布的方法或者软标签（对于分布而言一般用GT，加一个高斯窗）。

暂时不关注框和角点怎么产生，只聚焦于哪一个anchor适合回归当前的目标。在标签分配中我们一般把**匹配损失**（称作**cost**以区分和loss的不同）分为IoU损失、类别损失和GT与anchor的中心距离损失。鉴于我们回归的是角点，还可以视情况加入角点的回归损失。

不管用什么方法得到了外接或包围bbox，我们都可以用bbox和GT bbox的IOU替代角点形成的任意四边形与GT的IOU计算cost。虽然bbox的IOU和四边形IOU之间存在gap，但这并不会影响我们追求更大IOU的**优化目标**。显而易见的是，随着外接矩形框的IOU逐渐增大，四边形IOU在总体趋势上也是跟着增大的，只不过我们很难衡量两者之间严格的相关关系，以及用这种替换方法产生的损失函数是否可导、损失和IOU在不同情况下的趋势。并且，相对于任意四边形IOU的计算，这非常非常方便。即使是旋转矩形要计算IOU（如现在常用的skew iou），过程都非常复杂（还好有人已经实现了），对于NMS和标签分配都有很大的效率损失。并且，旋转IOU有一个很大的缺点就是**存在不可导**的地方。至于任意四边形的IOU，计算时要考虑的情况和细节就更多了。哈工大威海的同学自己编写了任意四边形的IOU以实现标签分配，值得敬佩。由于IOU loss基本已经替代了以往通过L1 loss等优化bbox的方法（就是评测和训练不一致），使用IOU loss的合理性基本不用质疑；此外，采用IOU loss之后，和之后的角点距离损失之间的**正交性**恰好也会提升，防止出现重复优化的情况。使用IOU损失还有一个好处，就是它拥有**尺度不变性**。还要考虑一个问题，就是使用的IOU损失类型，这会影响其和角点距离损失之间的正交性以及评估的有效性，笔者没有定性分析过也不好妄下结论，建议用D-IOU或S-IOU即可（新出的一些精心设计的IOU loss可能反倒不适合，因为其设计目标是**矩形框**检测，而这个实际上并不是我们的优化目标，siou都怕是一些矫枉过正的嫌疑）。

类别损失则有两种，颜色和图案。若使用了解耦检测头的设计，则特征不一致的情况会得到很大的缓解。若没有解耦，则在计算cost的时候就需要特别注意平衡IOU损失和类别损失了：定位准确的anchor往往没有很高的分类分数，反之亦然。这就需要在计算cost的时候采用更精细的策略，最好是通过系数衡量类别和定位的”掌握程度“，从而动态调制两者所占的比重。

GT和anchor之间的距离显然是一种直观的先验知识，尤其是point-based的网络基本都提出过/依赖于这种损失范式，我们相信那些优质的预测框应该位于anchor的中心，这也符合卷积核和感受野两者的**形状**在空间上对称的直觉。加入这部分cost也可以防止出现开头所述的”坏学生不被关注”，导致此anchor直接摆烂。

> 在某种程度上，对于基于anchor的算法，IOU损失会包括GT和anchor的距离损失。

最后是由于引入角点需要增加的角点距离损失。虽然这看似（不是看似而是就是）和IOU损失会有冲突，但是由于使用了外接矩形替代，两者相关性会下降。另外，IOU损失和距离损失在学习的不同阶段比重也是不同的（即使你没有加入调制系数）。对于关键点检测，我们常常使用wing-loss以保证不同角点之间的优化程度能够相互平衡，而在装甲板这种四个角点都差不多的情况下，其实不太需要担心不同角点回归失衡的情况。因此，smooth-L1足堪使用。若想要增大表现差的预测的损失，可以考虑加入L2。另外，我们同样应该保证损失的尺度不变性，对于不同尺度的目标应该乘上尺度相关的系数以修正目标尺度变化带来的度量不一致的影响（大尺度的目标，即使角点偏离10个像素也说明回归的足够好了；反之小尺寸的目标即使改变一个像素也会引起巨大的变化）。

讲完以上四种cost，应该考虑它们在总cost中所占的权重了。从总的角度看来，类别损失和IOU+角点损失应该势均力敌，距离损失大概为IOU损失的一半。这种想法也是经验性，不过有一些论文专门研究loss比重的问题，可以参考。为了适应网络在不同学习阶段的掌握程度，在不同的轮次应当采用不同的系数：目标检测网络早期学习注重类别后期注重回归精度应该是毋庸置疑的结果了；另外，颜色损失于学习初期可以给予更大的权重，之后逐渐下降但也不能忽略否则会出现红蓝部分的情况。距离损失在回归任务已经较好习得时也可以降低，因为此时每个anchor已经逐渐找到了自己应该预测的角点和bbox。至于IOU和角点，遵循总体原则即可，可以尝试让角点cost占比稍微超过IOU，毕竟IOU对于微小的角点移动不是那么的敏感。

> 毕竟这么多组对照笔者也不可能一个个试，都是经验或猜想（~~吹水~~）。有时间的卡多的同学可以多跑点消融实验，整理一个表格。

另外，Dynamic SR-CNN提出选取的top-k个anchor中的K也应该是可变的，在初期应该让尽可能多的anchor接受正样本监督，随着学习的逐步提升，在减小k的数量以更精细地调整网络中的参数，这也是一种可以尝试的策略。

#### 6.1.3.5. Loss

其实和标签分配差不多，只不过没有了GT和anchor的距离损失。最终的损失函数就是颜色+类别+IOU+角点。在这一部分，笔者推荐给角点的权重提高一些，大概为IOU损失的1~2倍。优化器推荐选用Adam，这是最稳妥、最不需要担心参数调试的方法了。如果不是自己编写的网络，是从其他算法中修改而来的，那么它原来应该就已经选用了默认优化器，拿来直接用就完事了。若你想要尝试一些新鲜的优化器，可以考虑使用Adam的数种改进如AdamW、NAdam和RAdam等，在pytorch中均有提供。如果你认为自己的炼丹技术以及炉火纯青，可以挑战一下SGD。

gradient clip和模型参数滑动平均也是推荐使用的训练策略，都能帮你在最大程度上稳定训练过程。训练初期sigmoid常常可能出现NAN，尝试使用tanh替换，或者加入一个梯度死区。激活函数尽量选择通用的RELU，可以尝试LeakyRELU。SiLU、Swish、ELU等**在某些平台、设备或推理框架中可能尚未支持**，检查确定可能之后再整花活。另外，**不要忘记在log和所有用到除法的地方加上$\epsilon $!** 

**另外需要注意的是标注角点的顺序。如果数据集中的点都是按某种顺序标注的那没什么问题，若是随机标注的则需要在分配标签、计算loss之前先为标签的角点或预测的角点进行排序！推荐顺序为tl、tr、dr、dl，从左上角开始顺时针排序。**

#### 6.1.3.6. 其他

- 数据增强

在装甲板的定位任务中，中间的数字/图案特征明显是起了非常重要的作用（尤其是解耦颜色和分类之后更甚，即使两者没有解耦，数字特征仍然是区分装甲板和其他类灯条的发光物体最重要的部分），而灯条则是区分数字/图案和其他类似物体（特别是场地中的定位标签、大块白色不明物体，都常常被识别成装甲板）。因此，针对这两种情况，推荐加入一些上述的**负样本**，如场地上的灯条、定位标签、没有贴贴纸的装甲板、熄灭的装甲板（可以作为其他类，颜色中加入熄灭）。

多尺度可以很好地增加多样性，根据数据集中的样本形状选择合适的缩放大小，和前述一样，除有特殊需求外最大尺度不宜超过画幅的1/4；小尺度的目标也要保证再resize后的图片里至少占有共40个以上的像素（COCO把长或宽小于32的目标视为小目标），除非有特殊需求。

旋转增强大概开到20~30，不排除数据集里本来就有一定角度的装甲板（爬坡、视角不同等），开太大容易学到倒的装甲板，甚至把一些场地光效和不知道是什么的玩意识别成装甲板。

**伸展和收缩**增强应该根据图像预处理的pipeline和相机的分辨率进行设置！这一点非常重要，此参数的设置取决于图像在被输入网络的时候是怎么被resize的。是直接不保留原图比例暴力resize成输入大小，还是保持比例加padding？抑或是保持一边的比例...... 伸缩的参数尽量和预处理设置的一致！

**务必关闭镜像增强**，因为数字和图案可是不会倒转的！

另外几个关于像素值的增强（对比度、色调、亮度、饱和度等），也要保持在**可能出现的场景范围内**。相机调参往往不会特别剧烈，而且至少要看清装甲板图案又不能让灯条过曝，因此这几种增强也要根据工况进行调整。饱和度注意不要调太低不然会编程灰色。尤其是色调，如果变化得太大可能会导致紫色装甲板被识别成红色或蓝色！（我们学校哨兵开局狂射对方前哨站的场景仍然历历在目...）

对于综合的数据增强如mixup、mosaic等，都可以尝试使用。

- pretrained model

一般指的是在大型通用数据集上预训练的分类模型（对我们来说就是backbone）。浅层卷积核学习到的都是一些简单的纹理和边缘特征，预训练模型相当于帮我们把这部分特征学好了。不过对于装甲板检测来说，更高层的特征几乎是用不到了。笔者建议将backbone的前两个stage冻结，开放剩下的部分以进行训练。一般来说，数据量越大，解冻的层数就越多。在浅层特征已经学完的情况下，整个网络往往会收敛得更快。不过也有论文指出，通过一些特殊的方法和精心设计，从头开始（train from scratch）同样能达到相同甚至更高的精度，并且在更短的时间内收敛，有兴趣的可以查阅相关论文。一般来说，还是推荐使用预训练模型的。开源的算法中在配置文件里可以修改是否使用预训练的参数。

如果刚刚上手，直接用预训练模型就完事了，冻结整个backbone（这一般也不需要你设置）只开放neck和head。若你修改了backbone的结构，其实仍然可以使用预训练模型，只需要将那些未修改的地方对应即可。pytorch保存权重的文件是以字典的形式存储的，对此感兴趣的同学可以在pytorch文档中查阅相关内容。

> 不知道有没有卡多的同学愿意用现有的所有数据集帮大家整一个预训练模型？

- knowledge distillation











